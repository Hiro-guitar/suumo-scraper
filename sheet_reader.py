import gspread
from google.oauth2.service_account import Credentials
from suumo_scrape import extract_conditions_from_url
from suumo_search_url import build_suumo_search_url
from suumo_checker import find_matching_property, check_company_name
import datetime
import pytz

# === è¨­å®š ===
SPREADSHEET_ID_SOURCE = '1oZKxfoZbFWzTfZvSU_ZVHtnWLDmJDYNd6MSfNqlB074'
SOURCE_RANGE = 'A:J'

SPREADSHEET_ID = '195OS2gb97TUJS8srYlqLT5QXuXU0zUZxmbeuWtsGQRY'
SHEET_NAME = 'Sheet1'

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
creds = Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
client = gspread.authorize(creds)

# === å…ƒã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿å–å¾— ===
def get_source_data():
    sheet = client.open_by_key(SPREADSHEET_ID_SOURCE).sheet1
    result = sheet.get(SOURCE_RANGE)
    return [(row[0], row[1], row[9]) for row in result if len(row) >= 10 and row[0] and row[9].startswith('http')]

# === ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ ===
target_sheet = client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)
source_data = get_source_data()
existing_data = target_sheet.get_all_values()

# === ãƒ˜ãƒƒãƒ€ãƒ¼ + å±¥æ­´ä¿æŒå‡¦ç† ===
header = existing_data[0] if existing_data else []
existing_rows = existing_data[1:] if len(existing_data) > 1 else []
existing_map = {(row[0], row[1], row[2]): idx for idx, row in enumerate(existing_rows, start=2)}

# === æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¸Šæ›¸ã + ä¸è¦è¡Œã¯å‰Šé™¤ ===
new_rows = []
row_mapping = {}  # æ–°ã—ã„rowç•ªå· â†’ å…ƒãƒ‡ãƒ¼ã‚¿index

for idx, row in enumerate(source_data, start=2):
    new_rows.append([row[0], row[1], row[2]])
    row_mapping[idx] = row

# ç‰©ä»¶åãƒ»éƒ¨å±‹ç•ªå·ãƒ»URLã ã‘æ›´æ–°ï¼ˆAã€œCåˆ—ï¼‰
target_sheet.resize(rows=1)  # ãƒ˜ãƒƒãƒ€ãƒ¼ä»¥å¤–ãƒªã‚»ãƒƒãƒˆ
target_sheet.update('A2', new_rows)

# ä¸è¶³åˆ—ã‚ã‚Œã°è¿½åŠ 
all_values = target_sheet.get_all_values()
max_col = max(len(r) for r in all_values if any(c.strip() for c in r))
col_index = max_col + 1
if target_sheet.col_count < col_index:
    target_sheet.add_cols(col_index - target_sheet.col_count)

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨˜å…¥
now = datetime.datetime.now(pytz.timezone("Asia/Tokyo"))
timestamp = now.strftime("%m-%d %H:%M")
target_sheet.update_cell(1, col_index, timestamp)

# === SUUMOãƒã‚§ãƒƒã‚¯ä¸€æ‹¬å‡¦ç† ===
search_urls = []
statuses = []

for i, (title, room_no, url) in enumerate(source_data, start=2):
    print(f"ğŸ”— å‡¦ç†ä¸­: {url}")
    result = extract_conditions_from_url(url)

    if result:
        print(f"ğŸ  ç‰©ä»¶å: {result.get('title', 'N/A')}")
        search_url = build_suumo_search_url(
            station_info=result['stations'],
            price=result['price'],
            area_max=result['area'],
            age_max=result['age'],
            floor_plan=result['floor_plan']
        )

        if search_url:
            print(f"ğŸ” æ¤œç´¢URL: {search_url}")
            detail_url = find_matching_property(search_url, result)

            if detail_url:
                if check_company_name(detail_url):
                    print("â­•ï¸ ãˆã»ã†ã¾ããŒæ²è¼‰ä¸­ï¼")
                    status = "â­•ï¸"
                else:
                    print("âŒ ä»–ç¤¾æ²è¼‰")
                    status = "âŒ"
            else:
                print("ğŸ” ä¸€è‡´ç‰©ä»¶ãªã—")
                status = ""
        else:
            print("âš ï¸ æ¤œç´¢URLä½œæˆå¤±æ•—")
            search_url = ""
            status = "URLå¤±æ•—"
    else:
        print("âš ï¸ æ¡ä»¶æŠ½å‡ºå¤±æ•—")
        search_url = ""
        status = "æŠ½å‡ºå¤±æ•—"

    search_urls.append([search_url])
    statuses.append([status])

# === ä¸€æ‹¬ã§æ›¸ãè¾¼ã¿ ===
start_row = 2
end_row = start_row + len(search_urls) - 1

# æ¤œç´¢URLã‚’ Dåˆ—ã«
target_sheet.update(f"D{start_row}:D{end_row}", search_urls)

# çµæœï¼ˆâ­•ï¸âŒï¼‰ã‚’å±¥æ­´åˆ—ã«
from gspread.utils import rowcol_to_a1
start_cell = rowcol_to_a1(start_row, col_index)
end_cell = rowcol_to_a1(end_row, col_index)
target_sheet.update(f"{start_cell}:{end_cell}", statuses)
