import gspread
from google.oauth2.service_account import Credentials
from suumo_scrape import extract_conditions_from_url
from suumo_search_url import build_suumo_search_url
from suumo_checker import find_matching_property, check_company_name
import datetime
import pytz
import time

# === è¨­å®š ===
SPREADSHEET_ID_SOURCE = '1oZKxfoZbFWzTfZvSU_ZVHtnWLDmJDYNd6MSfNqlB074'
SOURCE_RANGE = 'A:J'  # å…ƒã‚·ãƒ¼ãƒˆã‹ã‚‰å–å¾—ã™ã‚‹ç¯„å›²

SPREADSHEET_ID = '195OS2gb97TUJS8srYlqLT5QXuXU0zUZxmbeuWtsGQRY'
SHEET_NAME = 'Sheet1'

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
creds = Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
client = gspread.authorize(creds)

# === å…ƒã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ===
def get_source_data():
    sheet = client.open_by_key(SPREADSHEET_ID_SOURCE).sheet1
    result = sheet.get(SOURCE_RANGE)
    # ç‰©ä»¶å, éƒ¨å±‹ç•ªå·, æ²è¼‰ãƒšãƒ¼ã‚¸URLï¼ˆURLã¯10åˆ—ç›®ï¼index9ï¼‰
    return [(row[0], row[1], row[9]) for row in result if len(row) >= 10 and row[0] and row[9].startswith('http')]

# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
def main():
    target_sheet = client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

    # 1. å…ƒã‚·ãƒ¼ãƒˆã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
    source_data = get_source_data()
    source_keys = set(source_data)  # ï¼ˆç‰©ä»¶å,éƒ¨å±‹ç•ªå·,URLï¼‰ã®é›†åˆ

    # 2. æ—¢å­˜ã‚·ãƒ¼ãƒˆã®å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰
    existing_data = target_sheet.get_all_values()
    if len(existing_data) < 2:
        existing_rows = []
    else:
        existing_rows = existing_data[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯é™¤ã

    # 3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆä½œæˆ
    existing_keys = set()
    for row in existing_rows:
        if len(row) >= 3:
            existing_keys.add((row[0], row[1], row[2]))

    # 4. æ—¢å­˜ã«ã‚ã£ã¦ã‚½ãƒ¼ã‚¹ã«ãªã„è¡Œã¯å‰Šé™¤ï¼ˆä¸‹ã‹ã‚‰é †ã«ï¼‰
    rows_to_delete = []
    for i, row in enumerate(existing_rows, start=2):
        key = tuple(row[:3])
        if key not in source_keys:
            rows_to_delete.append(i)

    for row_idx in reversed(rows_to_delete):
        print(f"ğŸ—‘ï¸ è¡Œ {row_idx} ã‚’å‰Šé™¤")
        target_sheet.delete_rows(row_idx)
        time.sleep(1)  # Google Sheets APIåˆ¶é™å¯¾ç­–ã§è»½ãå¾…æ©Ÿ

    # 5. ã‚½ãƒ¼ã‚¹ã®ç‰©ä»¶ã‚’è¡Œå˜ä½ã§è¾æ›¸åŒ–ï¼ˆkey=(ç‰©ä»¶å,éƒ¨å±‹ç•ªå·,URL)ï¼‰
    source_dict = {key: key for key in source_data}

    # 6. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’keyâ†’è¡Œç•ªå·ãƒãƒƒãƒ—åŒ–ï¼ˆæ›´æ–°ç”¨ï¼‰
    existing_key_to_row = {}
    for i, row in enumerate(target_sheet.get_all_values()[1:], start=2):
        if len(row) >= 3:
            existing_key_to_row[(row[0], row[1], row[2])] = i

    # 7. ã‚½ãƒ¼ã‚¹ã«ã‚ã‚‹ç‰©ä»¶ã‚’ã€æ—¢å­˜ã®è¡Œç•ªå·ã«æ›¸ãè¾¼ã‚€ã‹ã€æ–°è¦è¡Œè¿½åŠ ã‹åˆ¤å®š
    max_row = len(target_sheet.get_all_values())
    for key in source_data:
        if key in existing_key_to_row:
            # æ—¢å­˜è¡Œã«æ›´æ–°
            row_num = existing_key_to_row[key]
            target_sheet.update(f"A{row_num}:C{row_num}", [list(key)])
            time.sleep(0.5)
        else:
            # æ–°è¦è¡Œè¿½åŠ ï¼ˆæœ«å°¾ï¼‰
            max_row += 1
            target_sheet.update(f"A{max_row}:C{max_row}", [list(key)])
            time.sleep(0.5)

    # 8. åˆ—æ•°ã€æ—¥æ™‚ãƒ©ãƒ™ãƒ«ã®æº–å‚™
    all_values = target_sheet.get_all_values()
    max_col = max((len(row) for row in all_values if any(cell.strip() for cell in row)), default=0)
    result_col_index = max_col + 1
    if target_sheet.col_count < result_col_index:
        target_sheet.add_cols(result_col_index - target_sheet.col_count)

    tokyo = pytz.timezone('Asia/Tokyo')
    now = datetime.datetime.now(tokyo)
    timestamp = now.strftime("%m-%d %H:%M")
    target_sheet.update_cell(1, result_col_index, timestamp)

    # 9. å„ç‰©ä»¶ã®SUUMOãƒã‚§ãƒƒã‚¯å‡¦ç†
    # å†åº¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆAã€œCåˆ—ï¼‰
    updated_data = target_sheet.get_all_values()[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã

    for i, row in enumerate(updated_data, start=2):
        if len(row) < 3:
            continue
        url = row[2].strip()
        if not url.startswith("http"):
            continue

        print(f"ğŸ”— å‡¦ç†ä¸­: {url}")
        result = extract_conditions_from_url(url)

        if result:
            print(f"ğŸ  ç‰©ä»¶å: {result.get('title', 'N/A')}")

            search_url = build_suumo_search_url(
                station_info=result['stations'],
                price=result['price'],
                area_max=result['area'],
                age_max=result['age'],
                floor_plan=result['floor_plan']
            )

            if search_url:
                print(f"ğŸ” æ¤œç´¢URL: {search_url}")
                # Dåˆ—ï¼ˆ4åˆ—ç›®ï¼‰ã«æ¤œç´¢URLæ›´æ–°
                target_sheet.update_cell(i, 4, search_url)
                time.sleep(0.3)

                detail_url = find_matching_property(search_url, result)

                if detail_url:
                    if check_company_name(detail_url):
                        print("â­•ï¸ ãˆã»ã†ã¾ããŒæ²è¼‰ä¸­ï¼")
                        target_sheet.update_cell(i, result_col_index, "â­•ï¸")
                    else:
                        print("âŒ ä»–ç¤¾æ²è¼‰")
                        target_sheet.update_cell(i, result_col_index, "âŒ")
                else:
                    print("ğŸ” ä¸€è‡´ç‰©ä»¶ãªã—")
                    target_sheet.update_cell(i, result_col_index, "")
            else:
                print("âš ï¸ æ¤œç´¢URLä½œæˆå¤±æ•—")
                target_sheet.update_cell(i, result_col_index, "URLå¤±æ•—")
        else:
            print("âš ï¸ æ¡ä»¶æŠ½å‡ºå¤±æ•—")
            target_sheet.update_cell(i, result_col_index, "æŠ½å‡ºå¤±æ•—")

        time.sleep(1)  # APIåˆ¶é™å¯¾ç­–ã‚†ã£ãŸã‚Šå¾…æ©Ÿ

if __name__ == "__main__":
    main()
