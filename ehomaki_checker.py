# ehomaki_checker.py

import gspread
from google.oauth2.service_account import Credentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time
from suumo_scrape import extract_conditions_from_url
from suumo_search_url import build_suumo_search_url

# ========== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š ==========
SPREADSHEET_ID = '195OS2gb97TUJS8srYlqLT5QXuXU0zUZxmbeuWtsGQRY'
SHEET_NAME = 'Sheet1'
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
creds = Credentials.from_service_account_file('credentials.json', scopes=SCOPES)
client = gspread.authorize(creds)
sheet = client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

# ========== Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š ==========
def get_driver():
    options = Options()
    options.add_argument('--headless')  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ã§å®Ÿè¡Œ
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    return webdriver.Chrome(options=options)

# ========== ç©ºã„ã¦ã‚‹æ¬¡ã®åˆ—ï¼ˆå³ç«¯ï¼‰ã‚’å–å¾— ==========
def get_next_available_col():
    header_row = sheet.row_values(1)
    return len(header_row) + 1

# ========== æ¤œç´¢çµæœã‹ã‚‰ãˆã»ã†ã¾ãç‰©ä»¶ã‚’æ¢ã™ ==========
def check_ehomaki_listing(search_url, expected_data):
    driver = get_driver()
    driver.get(search_url)
    time.sleep(2)

    listings = driver.find_elements(By.CSS_SELECTOR, 'div.cassetteitem')

    for listing in listings:
        try:
            # è³ƒæ–™ï¼ˆã€Œ8.9ã€ä¸‡å††ã®ã‚ˆã†ãªæ•°å­—ï¼‰
            price_text = listing.find_element(By.CSS_SELECTOR, '.cassetteitem_price .value').text.strip().replace('ä¸‡å††', '')
            # é–“å–ã‚Š
            floor_plan_text = listing.find_element(By.CSS_SELECTOR, '.cassetteitem_madori').text.strip()
            # å°‚æœ‰é¢ç©ï¼ˆä¾‹: "19.48mÂ²" â†’ "19.48"ï¼‰
            area_text = listing.find_element(By.CSS_SELECTOR, '.cassetteitem_menseki').text.strip().replace('mÂ²', '')
            # æœ€å¯„é§…ãƒ»å¾’æ­©åˆ†æ•°ã‚’å«ã‚€ãƒ–ãƒ­ãƒƒã‚¯
            station_text = listing.find_element(By.CSS_SELECTOR, '.cassetteitem_detail-text').text
            # ç¯‰å¹´æ•°ãªã©ã®æƒ…å ±
            age_text = listing.find_element(By.CSS_SELECTOR, '.cassetteitem_detail-col1').text

            # === å®Œå…¨ä¸€è‡´åˆ¤å®š ===
            match = (
                price_text == str(expected_data["price"]) and
                area_text == str(expected_data["area"]) and
                floor_plan_text == expected_data["floor_plan"] and
                any(s["station"] in station_text for s in expected_data["stations"]) and
                f"å¾’æ­©{expected_data['stations'][0]['distance']}åˆ†" in station_text and
                f"ç¯‰{expected_data['age']}å¹´" in age_text
            )

            # ã“ã“ã«printã‚’å…¥ã‚Œã¦ãƒ‡ãƒãƒƒã‚°
            print("=== æ¯”è¼ƒä¸­ ===")
            print("è³ƒæ–™:", price_text, "æœŸå¾…å€¤:", expected_data["price"])
            print("é¢ç©:", area_text, "æœŸå¾…å€¤:", expected_data["area"])
            print("é–“å–ã‚Š:", floor_plan_text, "æœŸå¾…å€¤:", expected_data["floor_plan"])
            print("é§…åå«ã‚€ãƒ†ã‚­ã‚¹ãƒˆ:", station_text)
            print("æœŸå¾…é§…åä¸€è¦§:", [s["station"] for s in expected_data["stations"]])
            print("å¾’æ­©åˆ†æ•°åˆ¤å®šæ–‡å­—åˆ—:", f"å¾’æ­©{expected_data['stations'][0]['distance']}åˆ†")
            print("ç¯‰å¹´æ•°ãƒ†ã‚­ã‚¹ãƒˆ:", age_text)
            print("åˆ¤å®šçµæœ:", match)
            print("-------------")

            if not match:
                continue

            # === è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ç¤¾åã‚’ç¢ºèª ===
            detail_link = listing.find_element(By.CSS_SELECTOR, 'a.js-cassette_link_href').get_attribute('href')
            driver.get(detail_link)
            time.sleep(1.5)

            if "åˆåŒä¼šç¤¾ãˆã»ã†ã¾ã" in driver.page_source:
                driver.quit()
                return True

            driver.back()
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            continue

    driver.quit()
    return False

# æ–°ã—ã„åˆ—ã‚’1åˆ—å¢—ã‚„ã—ã¦æ›¸ãè¾¼ã‚€
def append_new_column_and_write(row, value):
    header = sheet.row_values(1)
    current_cols = len(header)
    next_col = current_cols + 1

    # åˆ—æ•°ãŒè¶³ã‚Šãªã‘ã‚Œã°æ‹¡å¼µ
    sheet.resize(cols=next_col)

    # æ›¸ãè¾¼ã¿
    sheet.update_cell(row, next_col, value)

# ========== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==========
def main():
    urls = sheet.col_values(3)[1:]  # Cåˆ—ã®URLï¼ˆ2è¡Œç›®ã‹ã‚‰ï¼‰
    next_col = get_next_available_col()

    for idx, url in enumerate(urls, start=2):
        if not url.strip().startswith("http"):
            continue

        print(f"ğŸ” å‡¦ç†ä¸­: Row {idx} - {url}")
        data = extract_conditions_from_url(url)
        if not data:
            print("âš ï¸ æŠ½å‡ºå¤±æ•—")
            append_new_column_and_write(idx, "ã‚¨ãƒ©ãƒ¼")
            continue

        search_url = build_suumo_search_url(
            station_info=data['stations'],
            price=data['price'],
            area_max=data['area'],
            age_max=data['age'],
            floor_plan=data['floor_plan']
        )

        if not search_url:
            print("âš ï¸ æ¤œç´¢URLç”Ÿæˆå¤±æ•—")
            append_new_column_and_write(idx, "URLã‚¨ãƒ©ãƒ¼")
            continue

        is_listed = check_ehomaki_listing(search_url, data)
        result = "â­•ï¸" if is_listed else ""
        append_new_column_and_write(idx, result)

        print(f"âœ… çµæœ: {result or 'è¦‹ã¤ã‹ã‚‰ãš'}")

if __name__ == "__main__":
    main()
